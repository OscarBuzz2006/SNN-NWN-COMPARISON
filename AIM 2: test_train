import torch
import torch.nn as nn
import snntorch as snn
from snntorch import surrogate
from snntorch import functional as SF
from snntorch import utils
import numpy as np
from collections import Counter
import matplotlib.pyplot as plt

# Training Parameters
batch_size=128
data_path='/tmp/data/mnist'
num_classes = 10  # MNIST has 10 output classes

device = torch.device("cuda") if torch.cuda.is_available() else torch.device('mps') if torch.backends.mps.is_available() else torch.device("cpu")

# Testing and training data
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.Grayscale(),
    transforms.ToTensor()
])

mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)
mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)

train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)



## REQUIRED POPULATION CODING ##


def powerlaw_conv(data, alpha=1.5, min_val=1e-7):
    uniform_random = torch.rand_like(data)
    
    if alpha != 1.0:
        powerlaw_samples = (1 - uniform_random) ** (-1/(alpha - 1))
    else:
        powerlaw_samples = -torch.log(uniform_random)
    
    # FIXED: Use power-law samples directly as modulation (don't rescale)
    spike_probability = data * torch.clamp(1.0 / powerlaw_samples, 0, 2)  # Invert for proper weighting
    spike_probability = torch.clamp(spike_probability, 0, 1)
    spike_data = torch.bernoulli(spike_probability)
    
    return spike_data

def modified_rate_encoding(data, num_steps=10, method='powerlaw', **kwargs):
    """Drop-in replacement for snntorch.spikegen.rate() with power-law option.
    
    Args:
        data: Input tensor
        num_steps: Number of time steps
        method: 'poisson' (original) or 'powerlaw' (critical)
        **kwargs: Additional arguments for power-law generation
    
    Returns:
        Spike tensor with chosen statistics
    """
    # Repeat data across time dimension
    time_data = data.repeat(tuple([num_steps] + [1] * len(data.shape)))
    
    if method == 'poisson':
        # Original snnTorch behavior
        spike_data = torch.bernoulli(torch.clamp(time_data, 0, 1))
    elif method == 'powerlaw':
        # Your critical avalanche modification
        spike_data = powerlaw_conv(time_data, **kwargs)
    else:
        raise ValueError(f"Unknown method: {method}")
    return spike_data

##ABOVE IS COPIED FROM SNN_MNIST_samples.PY FOR DATA LOADING AND MODIFICATION; BELOW IS THE CORE SNN TRAINING AND TESTING CODE##

# Test accuracy function with proper time-step handling
def test_accuracy(test_loader, net, num_steps, population_code=True, num_classes=10):
    with torch.no_grad():
        net.eval()
        test_acc = []
        
        for data, targets in iter(test_loader):
            data = data.to(device)
            targets = targets.to(device)
            
            # POWER-LAW ENCODING - returns [num_steps, batch_size, features]
            spike_data = modified_rate_encoding(data.view(data.size(0), -1), 
                                              num_steps=num_steps, 
                                              method='powerlaw', 
                                              alpha=1.5)
            
            
            # Forward pass with proper time-step handling
            spk_rec, mem_rec = forward_pass(net, spike_data)
            
            # Population coding accuracy calculation
            if population_code:
                acc = SF.accuracy_rate(spk_rec, targets, population_code=True, num_classes=num_classes)
            else:
                acc = SF.accuracy_rate(spk_rec, targets)
                
            test_acc.append(acc)
            
        net.train()
        return np.mean(test_acc)

# Forward pass function that handles time steps correctly
def forward_pass(net, spike_data):
    """
    Forward pass through network handling time dimension properly
    spike_data shape: [num_steps, batch_size, features]
    """
    spk_rec = []
    mem_rec = []
    
    # Initialize hidden states
    utils.reset(net)
    
    # Loop through each time step
    for step in range(spike_data.size(0)):  # Loop over time dimension
        spk_out, mem_out = net(spike_data[step])  # Feed [batch_size, features] to network
        spk_rec.append(spk_out)
        mem_rec.append(mem_out)
    
    return torch.stack(spk_rec), torch.stack(mem_rec)

# TRAINING OPTION 1: Custom training loop with explicit power-law encoding
def train_powerlaw_population(net, train_loader, num_epochs=5):
    print("Training with Poisson Population Coding (Option 1)")
    print("="*60)
    
    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")
        
        epoch_loss = 0
        num_batches = 0
        
        for batch_idx, (data, targets) in enumerate(train_loader):
            data = data.to(device)
            targets = targets.to(device)
            
            # POWER-LAW ENCODING - returns [num_steps, batch_size, features]
            spike_data = modified_rate_encoding(data.view(data.size(0), -1), 
                                              num_steps=num_steps, 
                                              method='powerlaw', 
                                              alpha=1.5)
            
            # Forward pass with proper time handling
            spk_rec, mem_rec = forward_pass(net, spike_data)
            
            # Loss calculation - SF.mse_count_loss expects [time, batch, neurons]
            loss_val = loss_fn(spk_rec, targets)
            
            # Backpropagation
            optimizer.zero_grad()
            loss_val.backward()
            optimizer.step()
            
            epoch_loss += loss_val.item()
            num_batches += 1
            
            # Print progress every 100 batches
            if batch_idx % 100 == 0:
                print(f"  Batch {batch_idx}/{len(train_loader)}, Loss: {loss_val.item():.6f}")
        
        # Calculate average loss for epoch
        avg_loss = epoch_loss / num_batches
        
        # Test accuracy
        test_acc = test_accuracy(test_loader, net, num_steps, 
                               population_code=True, num_classes=num_classes)
        
        print(f"Average Loss: {avg_loss:.6f}")
        print(f"Test Accuracy: {test_acc*100:.3f}%")
        print("-" * 40)
    
    return net


# Network parameters (from tutorial)
num_inputs = 28*28
num_hidden = 128
num_outputs = 10
num_steps = 10

# Spiking neuron parameters
beta = 0.9  # neuron decay rate 
grad = surrogate.fast_sigmoid()

# Population coding setup - 500 output neurons, 50 per class
pop_outputs = 500

# Build the population coding network (exact tutorial structure)
net_pop = nn.Sequential(nn.Flatten(),
                        nn.Linear(num_inputs, num_hidden),
                        snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True),
                        nn.Linear(num_hidden, pop_outputs),
                        snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True, output=True)
                        ).to(device)

print("Network Architecture:")
print(net_pop)
print(f"Population coding: {pop_outputs} output neurons, {pop_outputs//num_classes} neurons per class")

# Loss function and optimizer (from tutorial)
loss_fn = SF.mse_count_loss(correct_rate=1.0, incorrect_rate=0.0, 
                           population_code=True, num_classes=num_classes)
optimizer = torch.optim.Adam(net_pop.parameters(), lr=2e-3, betas=(0.9, 0.999))
# Run the training
print("Training Poisson Population Coding SNN...")
print(f"Using {device}")
print(f"Time steps: {num_steps}")

# Train the network
trained_net = train_powerlaw_population(net_pop, train_loader, num_epochs=5)

print("\nTraining completed!")

# Final evaluation
print("\nFinal Results:")
final_accuracy = test_accuracy(test_loader, trained_net, num_steps, 
                             population_code=True, num_classes=num_classes)
print(f"Final Test Accuracy with Poisson Population Coding: {final_accuracy*100:.3f}%")

def detect_spike_bursts_simple(spike_data):
    """
    Simple burst detection for criticality analysis
    
    Args:
        spike_data: [num_steps, batch_size, neurons] or [num_steps, neurons]
    
    Returns:
        List of burst durations
    """
    # If batch dimension exists, sum over batch and neurons for population activity
    if len(spike_data.shape) == 3:
        activity = spike_data.sum(dim=(1, 2))  # [num_steps]
    else:
        activity = spike_data.sum(dim=1)  # [num_steps]
    
    # Convert to binary: active (>0) or silent (=0)
    active_bins = (activity > 0).int()
    
    # Detect bursts (continuous periods of activity)
    burst_durations = []
    in_burst = False
    current_duration = 0
    
    for t in range(len(active_bins)):
        if active_bins[t] > 0 and not in_burst:
            # Start of new burst
            in_burst = True
            current_duration = 1
        elif active_bins[t] > 0 and in_burst:
            # Continue burst
            current_duration += 1
        elif active_bins[t] == 0 and in_burst:
            # End of burst
            in_burst = False
            if current_duration > 0:
                burst_durations.append(current_duration)
            current_duration = 0
    
    # Handle case where data ends during a burst
    if in_burst and current_duration > 0:
        burst_durations.append(current_duration)
    
    return burst_durations

def plot_criticality_analysis(spike_data, title="Criticality Analysis", save_path=None):
    """
    Plot burst duration vs frequency to assess criticality
    
    Args:
        spike_data: Spike tensor from your network
        title: Plot title
        save_path: Optional path to save plot
    """
    # Detect burst durations
    burst_durations = detect_spike_bursts_simple(spike_data)
    
    if len(burst_durations) == 0:
        print("No bursts detected in spike data!")
        return
    
    # Count frequency of each duration
    duration_counts = Counter(burst_durations)
    durations = sorted(duration_counts.keys())
    frequencies = [duration_counts[d] for d in durations]
    
    # Create log-log plot
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Linear plot
    ax1.bar(durations, frequencies, alpha=0.7, color='blue')
    ax1.set_xlabel('Burst Duration (time steps)')
    ax1.set_ylabel('Frequency')
    ax1.set_title(f'{title} - Linear Scale')
    ax1.grid(True, alpha=0.3)
    
    # Log-log plot for power-law detection
    ax2.loglog(durations, frequencies, 'o-', color='red', markersize=8, linewidth=2)
    ax2.set_xlabel('Log Burst Duration')
    ax2.set_ylabel('Log Frequency')
    ax2.set_title(f'{title} - Log-Log Scale')
    ax2.grid(True, alpha=0.3)
    
    # Fit power law in log space for criticality assessment
    if len(durations) > 3:
        log_durations = np.log10(durations)
        log_frequencies = np.log10(frequencies)
        
        # Linear fit in log space: log(y) = -α * log(x) + const
        slope, intercept = np.polyfit(log_durations, log_frequencies, 1)
        
        # Plot fit line
        fit_line = 10**(slope * log_durations + intercept)
        ax2.plot(durations, fit_line, '--', color='black', linewidth=2, 
                label=f'Power law fit: α = {-slope:.2f}')
        ax2.legend()
        
        # Criticality assessment
        critical_alpha = 2.0  # Theoretical value for critical avalanches
        print(f"\nCriticality Analysis for {title}:")
        print(f"Power-law exponent (α): {-slope:.3f}")
        print(f"Theoretical critical value: {critical_alpha:.1f}")
        print(f"Distance from criticality: {abs(-slope - critical_alpha):.3f}")
        
        if abs(-slope - critical_alpha) < 0.3:
            print("✓ Close to critical state!")
        elif -slope > critical_alpha:
            print("⚠ Sub-critical (too stable)")
        else:
            print("⚠ Super-critical (too excitable)")
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Plot saved to {save_path}")
    
    plt.show()
    
    # Print basic statistics
    print(f"\nBurst Statistics:")
    print(f"Total bursts detected: {len(burst_durations)}")
    print(f"Mean burst duration: {np.mean(burst_durations):.2f}")
    print(f"Max burst duration: {max(burst_durations)}")
    print(f"Min burst duration: {min(burst_durations)}")
    
    return burst_durations, durations, frequencies

# Example usage with your power-law network data
def evaluate_network_criticality(net, test_loader, num_steps=25):
    """
    Evaluate criticality of your trained network
    """
    print("Evaluating Network Criticality...")
    
    # Get some test data
    with torch.no_grad():
        net.eval()
        
        # Collect spike data from multiple batches
        all_spike_data = []
        
        for batch_idx, (data, targets) in enumerate(test_loader):
            if batch_idx >= 5:  # Just use 5 batches for analysis
                break
                
            data = data.to(device)
            
            # Power-law encoding
            spike_data = modified_rate_encoding(data.view(data.size(0), -1), 
                                              num_steps=num_steps, 
                                              method='powerlaw', 
                                              alpha=1.5)
            
            # Get network spikes (analyze hidden layer activity)
            spk_rec, mem_rec = forward_pass(net, spike_data)
            all_spike_data.append(spk_rec)
        
        # Concatenate all spike data
        combined_spikes = torch.cat(all_spike_data, dim=1)  # [time, total_batch, neurons]
        
        # Analyze criticality
        plot_criticality_analysis(combined_spikes, 
                                title="Power-Law Network Hidden Layer Activity",
                                save_path="network_criticality.png")

# Simple function to analyze just your encoded input data
def analyze_input_criticality():
    """
    Quick analysis of your power-law encoded input spikes
    """
    # Get a batch of test data
    test_batch = next(iter(test_loader))
    data, targets = test_batch
    data = data[:32].to(device)  # Use 32 samples
    
    # Power-law encoding
    powerlaw_spikes = modified_rate_encoding(data.view(data.size(0), -1), 
                                           num_steps=50,  # More time steps for better analysis
                                           method='powerlaw', 
                                           alpha=1.5)
    
    # Poisson encoding for comparison
    poisson_spikes = modified_rate_encoding(data.view(data.size(0), -1), 
                                          num_steps=50, 
                                          method='poisson')
    
    # Analyze both
    print("Analyzing INPUT spike patterns...")
    
    plot_criticality_analysis(powerlaw_spikes, 
                            title="Power-Law Input Encoding",
                            save_path="input_powerlaw_criticality.png")
    
    plot_criticality_analysis(poisson_spikes, 
                            title="Poisson Input Encoding", 
                            save_path="input_poisson_criticality.png")

# Run the analysis
if __name__ == "__main__":
    # Analyze input encoding criticality
    analyze_input_criticality()
    
    # If you have a trained network, analyze network criticality
    # evaluate_network_criticality(trained_net, test_loader)
