import torch
import torch.nn as nn
import snntorch as snn
from snntorch import surrogate
from snntorch import functional as SF
from snntorch import utils
import numpy as np
from collections import Counter
import matplotlib.pyplot as plt
import powerlaw

# Testing and training data
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Training Parameters
batch_size=128
data_path='/tmp/data/mnist'
num_classes = 10  # MNIST has 10 output classes

device = torch.device("cuda") if torch.cuda.is_available() else torch.device('mps') if torch.backends.mps.is_available() else torch.device("cpu")

transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.ToTensor()
])

mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)
mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)

train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)


## REQUIRED POPULATION CODING ##

def powerlaw_conv(data, alpha=1.5):
    uniform_random = torch.rand_like(data)
    
    powerlaw_samples = -torch.log(uniform_random)
    
    # FIXED: Use power-law samples directly as modulation (don't rescale)
    spike_probability = data * torch.clamp(1.0 / powerlaw_samples, 0, 2)
    spike_probability = torch.clamp(spike_probability, 0, 1)
    spike_data = torch.bernoulli(spike_probability)
    
    return spike_data

def modified_rate_encoding(data, num_steps=10, method='powerlaw', **kwargs):
    """Drop-in replacement for snntorch.spikegen.rate() with power-law option.
    
    Returns:
        Spike tensor with chosen statistics
    """
    # Repeat data across time dimension
    time_data = data.repeat(tuple([num_steps] + [1] * len(data.shape)))
    
    if method == 'poisson':
        # Original snnTorch behavior, SAME AS THE RATE_CONV() FUNCTION
        spike_data = torch.bernoulli(torch.clamp(time_data, 0, 1))
    elif method == 'powerlaw':
        # Your critical avalanche modification
        spike_data = powerlaw_conv(time_data, **kwargs)
    else:
        raise ValueError(f"Unknown method: {method}")
    return spike_data

##ABOVE IS COPIED FROM SNN_MNIST_samples.PY (spike visualisation) FOR DATA LOADING AND MODIFICATION; BELOW IS THE CORE SNN TRAINING AND TESTING CODE##

# Test accuracy function FROM SNNTORCH, MODIFIED FOR MANUAL POPULATION CODING
def test_accuracy(test_loader, net, num_steps, population_code=True, num_classes=10):
    with torch.no_grad():
        net.eval()
        test_acc = []
        
        for data, targets in iter(test_loader):
            data = data.to(device)
            targets = targets.to(device)
            
        
            # POWER-LAW ENCODING - returns [num_steps, batch_size, features]
            spike_data = modified_rate_encoding(data.view(data.size(0), -1), 
                                              num_steps=num_steps, 
                                              method='poisson')
            
            
            # Forward pass with proper time-step handling
            spk_rec, mem_rec = forward_pass(net, spike_data)
            
            # Population coding accuracy calculation
            if population_code:
                acc = SF.accuracy_rate(spk_rec, targets, population_code=True, num_classes=num_classes)
            else:
                acc = SF.accuracy_rate(spk_rec, targets)
                
            test_acc.append(acc)
            
        net.train()
        return np.mean(test_acc)

# Forward pass function that handles time steps correctly
def forward_pass(net, spike_data):
    """
    Forward pass through network handling time dimension properly
    spike_data shape: [num_steps, batch_size, features]
    """
    spk_rec = []
    mem_rec = []
    
    # Initialize hidden states
    utils.reset(net)
    
    # Loop through each time step
    for step in range(spike_data.size(0)):  # Loop over time dimension
        spk_out, mem_out = net(spike_data[step])  # Feed [batch_size, features] to network
        spk_rec.append(spk_out)
        mem_rec.append(mem_out)
    
    return torch.stack(spk_rec), torch.stack(mem_rec)

# TRAINING OPTION 1: Custom training loop with explicit power-law encoding
def train_powerlaw_population(net, train_loader, num_epochs=5, num_steps=10, num_classes=10):
    print("Training with PL Population Coding (Option 1)")
    print("="*60)

    # Define loss function and optimizer inside this function
    loss_fn = SF.mse_count_loss(correct_rate=1.0, incorrect_rate=0.0,
                                population_code=True, num_classes=num_classes)
    optimizer = torch.optim.Adam(net.parameters(), lr=2e-3, betas=(0.9, 0.999))
    
    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")
        
        epoch_loss = 0
        num_batches = 0
        
        for batch_idx, (data, targets) in enumerate(train_loader):
            data = data.to(device)
            targets = targets.to(device)
            
            
            # POWER-LAW ENCODING - returns [num_steps, batch_size, features]
            spike_data = modified_rate_encoding(data.view(data.size(0), -1), 
                                              num_steps=num_steps, 
                                              method='poisson')
            
            # Forward pass with proper time handling
            spk_rec, mem_rec = forward_pass(net, spike_data)
            
            # Loss calculation - SF.mse_count_loss expects [time, batch, neurons]
            loss_val = loss_fn(spk_rec, targets)
            
            # Backpropagation
            optimizer.zero_grad()
            loss_val.backward()
            optimizer.step()
            
            epoch_loss += loss_val.item()
            num_batches += 1
            
            # Print progress every 100 batches
            if batch_idx % 100 == 0:
                print(f"  Batch {batch_idx}/{len(train_loader)}, Loss: {loss_val.item():.6f}")
        
        # Calculate average loss for epoch
        avg_loss = epoch_loss / num_batches
        
        # Test accuracy
        test_acc = test_accuracy(test_loader, net, num_steps, 
                               population_code=True, num_classes=num_classes)
        
        print(f"Average Loss: {avg_loss:.6f}")
        print(f"Test Accuracy: {test_acc*100:.3f}%")
        print("-" * 40)
    
    return net


def train_population_coding_snn(train_loader, test_loader, device, num_classes=10):
    """
    Train a population coding spiking neural network.
        
    Returns:
        trained_net: The trained network
        final_accuracy: Final test accuracy
    """
    
    # Network parameters (from tutorial)
    num_inputs = 28*28
    num_hidden = 128
    num_outputs = 10
    num_steps = 10
    
    # Spiking neuron parameters
    beta = 0.9  # neuron decay rate
    grad = surrogate.fast_sigmoid()  # surrogate gradient
    
    # Population coding setup - 500 output neurons, 50 per class
    pop_outputs = 500
    
    # Build the population coding network (exact tutorial structure)
    net_pop = nn.Sequential(nn.Flatten(),
                            nn.Linear(num_inputs, num_hidden),
                            snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True),
                            nn.Linear(num_hidden, pop_outputs),
                            snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True, output=True)
                            ).to(device)
    
    utils.reset(net_pop)  # Clear retained hidden states
    
    print("Network Architecture:")
    print(net_pop)
    print(f"Population coding: {pop_outputs} output neurons, {pop_outputs//num_classes} neurons per class")
    
    # Loss function and optimizer (from tutorial)
    loss_fn = SF.mse_count_loss(correct_rate=1.0, incorrect_rate=0.0,
                                population_code=True, num_classes=num_classes)
    optimizer = torch.optim.Adam(net_pop.parameters(), lr=2e-3, betas=(0.9, 0.999))
    
    print("Training PL Population Coding SNN...")
    print(f"Time steps: {num_steps}")
    
    # Train the network
    trained_net = train_powerlaw_population(net_pop, train_loader, num_epochs=5, num_steps=num_steps)
    
    print("\nTraining completed!")
    
    # Final evaluation
    print("\nFinal Results:")
    final_accuracy = test_accuracy(test_loader, trained_net, num_steps,
                                  population_code=True, num_classes=num_classes)
    print(f"Final Test Accuracy with PL Population Coding: {final_accuracy*100:.3f}%")
    
    return trained_net, final_accuracy


# Example usage in main:
if __name__ == "__main__":
    # PERFORMANCE EVALUATION
    trained_network, accuracy = train_population_coding_snn(train_loader, test_loader, device)
