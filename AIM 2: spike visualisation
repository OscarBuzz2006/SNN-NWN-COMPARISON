import torch
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import snntorch.spikeplot as splt
from matplotlib.animation import PillowWriter

# Training Parameters
batch_size=128
data_path='/tmp/data/mnist'
num_classes = 10  # MNIST has 10 output classes

device = torch.device("cuda") if torch.cuda.is_available() else torch.device('mps') if torch.backends.mps.is_available() else torch.device("cpu")

# Testing and training data
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.Grayscale(),
    transforms.ToTensor()
])

mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)
mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)

train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)



## REQUIRED POPULATION CODING (relevant for Friday 15 Aug) ##


def powerlaw_conv(data, alpha=1.5):
    """Power-law replacement for torch.bernoulli()"""
    clipped_data = torch.clamp(data, min=1e-7, max=1)
    
    # Use PyTorch's Pareto distribution
    from torch.distributions import Pareto
    pareto_dist = Pareto(scale=1e-7, alpha=alpha-1)  # alpha-1 because Pareto uses different parameterization
    
    # Sample from power law and use as threshold modulation
    powerlaw_samples = pareto_dist.sample(clipped_data.shape)
    
    # Modulate spike probability
    spike_probability = clipped_data / powerlaw_samples
    spike_probability = torch.clamp(spike_probability, 0, 1)
    
    return torch.bernoulli(spike_probability)

def modified_rate_encoding(data, num_steps=10, method='powerlaw', **kwargs):
    """Drop-in replacement for snntorch.spikegen.rate() with power-law option.
    
    Args:
        data: Input tensor
        num_steps: Number of time steps
        method: 'poisson' (original) or 'powerlaw' (critical)
        **kwargs: Additional arguments for power-law generation
    
    Returns:
        Spike tensor with chosen statistics
    """
    # Repeat data across time dimension
    time_data = data.repeat(tuple([num_steps] + [1] * len(data.shape)))
    
    if method == 'poisson':
        # Original snnTorch behavior
        spike_data = torch.bernoulli(torch.clamp(time_data, 0, 1))
    elif method == 'powerlaw':
        # Your critical avalanche modification
        spike_data = powerlaw_conv(time_data, **kwargs)
    else:
        raise ValueError(f"Unknown method: {method}")
    return spike_data

## PLOTTING DATA SHAPE (hist) ##

def plot_spike_histogram(spike_data, dt_steps=10, save_path="spike_hist"):
    
    counts = spike_data.sum(dim=0).flatten().cpu().numpy()
    
    # Plot histogram
    plt.figure(figsize=(8, 5))
    plt.hist(counts, bins=int(counts.max()) + 1, density=True, alpha=0.7, edgecolor='black')
    plt.xlabel(f'Spikes in dt={dt_steps}')
    plt.ylabel('Probability')
    plt.title('Spike Count Distribution')
    plt.axvline(counts.mean(), color='red', linestyle='--', label=f'Mean: {counts.mean():.2f}')
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"{save_path}.png", dpi=150, bbox_inches='tight')
    plt.show()


## VISUALISING DATA ##


# Extract a sample of digits (e.g., one of each class)
sample_images = []
sample_labels = []
num_samples_per_class = 1

for i in range(len(mnist_train)):
    img, label = mnist_train[i]
    if label not in sample_labels:
        sample_images.append(img)
        sample_labels.append(label)
        if len(sample_labels) == 10:  # Got one of each class
            break

print(f"Collected {len(sample_images)} sample images with labels: {sample_labels}")

# Process each collected sample image
for i, (img, label) in enumerate(zip(sample_images, sample_labels)):
    print(f"\nProcessing Digit {label} (Image {i+1}/10)...")
    
    # Prepare image data
    img_flat = img.squeeze().view(-1)  # shape: [784]
    num_steps = 50
    
    # Original Poisson encoding
    poisson_spikes = modified_rate_encoding(img_flat.unsqueeze(0), 
                                           num_steps=num_steps, method='poisson')
    
    # Power-law encoding for critical avalanches  
    critical_spikes = modified_rate_encoding(img_flat.unsqueeze(0), 
                                           num_steps=num_steps, 
                                           method='powerlaw', alpha=1.5)
    
    print(f"  Poisson  - mean: {poisson_spikes.float().mean().item():.6f}, sum: {poisson_spikes.sum().item()}")
    print(f"  Critical - mean: {critical_spikes.float().mean().item():.6f}, sum: {critical_spikes.sum().item()}")
    
    # Create GIFs for this digit
    
    # Poisson GIF
    fig, ax = plt.subplots()
    ax.set_title(f'Poisson Encoding - Digit {label}')
    anim = splt.animator(poisson_spikes[:, 0].view(num_steps, 28, 28), fig, ax)
    anim.save(f"poisson_digit_{label}.gif", writer=PillowWriter(fps=10))
    plt.close(fig)
    
    # Critical GIF  
    fig, ax = plt.subplots()
    ax.set_title(f'Critical Encoding - Digit {label}')
    anim = splt.animator(critical_spikes[:, 0].view(num_steps, 28, 28), fig, ax)
    anim.save(f"critical_digit_{label}.gif", writer=PillowWriter(fps=10))
    plt.close(fig)
    
    print(f"  Generated: poisson_digit_{label}.gif and critical_digit_{label}.gif")

print(f"\nGenerated 20 GIF files total:")
print("- 10 Poisson encoding GIFs (poisson_digit_X.gif)")  
print("- 10 Critical encoding GIFs (critical_digit_X.gif)")


if __name__ == "__main__":
    # Collect data over 10 batches
    all_data = []
    all_targets = []
    
    data_iter = iter(test_loader)
    for i in range(10):
        data, targets = next(data_iter)
        all_data.append(data)
        all_targets.append(targets)
    
    # Concatenate all batches
    combined_data = torch.cat(all_data, dim=0)
    combined_targets = torch.cat(all_targets, dim=0)

    # Optional: generate for specific numbers
    #mask = (combined_targets == 1)
    #combined_data = combined_data[mask]
    
    # Generate spikes for all data
    spikes = modified_rate_encoding(combined_data, num_steps=10, method='powerlaw', alpha=1.5)
    plot_spike_histogram(spikes, dt_steps=1)



